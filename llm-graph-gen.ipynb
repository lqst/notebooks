{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some fun with unstructured text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "import xml.etree.ElementTree as et "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv parameters\n",
    "arxiv_endpoint_template='http://export.arxiv.org/api/query?search_query=all:graph%20machine%20learning&start={start_index}&max_results={max_results}'\n",
    "start_index=0\n",
    "max_results=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = arxiv_endpoint_template.format(start_index=start_index, max_results = max_results)\n",
    "raw = urllib.request.urlopen(url)\n",
    "data = raw.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtree = et.fromstring(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://arxiv.org/abs/2201.01288v1',\n",
       " 'title': 'Automated Graph Machine Learning: Approaches, Libraries and Directions',\n",
       " 'published': '2022-01-04T18:31:31Z',\n",
       " 'summary': \"  Graph machine learning has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To tackle the challenge, automated graph machine learning,\\nwhich aims at discovering the best hyper-parameter and neural architecture\\nconfiguration for different graph tasks/data without manual design, is gaining\\nan increasing number of attentions from the research community. In this paper,\\nwe extensively discuss automated graph machine approaches, covering\\nhyper-parameter optimization (HPO) and neural architecture search (NAS) for\\ngraph machine learning. We briefly overview existing libraries designed for\\neither graph machine learning or automated machine learning respectively, and\\nfurther in depth introduce AutoGL, our dedicated and the world's first\\nopen-source library for automated graph machine learning. Last but not least,\\nwe share our insights on future research directions for automated graph machine\\nlearning. This paper is the first systematic and comprehensive discussion of\\napproaches, libraries as well as directions for automated graph machine\\nlearning.\\n\",\n",
       " 'categories': ['cs.LG', 'cs.AI'],\n",
       " 'authors': ['Xin Wang', 'Ziwei Zhang', 'Wenwu Zhu']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = { 'Atom' : 'http://www.w3.org/2005/Atom' }\n",
    "papers=[] \n",
    "\n",
    "for node in xtree:\n",
    "  if node.tag.endswith('entry'):\n",
    "    #print(node.tag, node.attrib)\n",
    "    paperId = node.find('Atom:id', ns).text\n",
    "    categories = [];\n",
    "\n",
    "    for cat in node.findall('Atom:category', ns):\n",
    "      categories.append(cat.get('term'))\n",
    "    \n",
    "    authors=[]\n",
    "    for aut in node.findall('Atom:author', ns):\n",
    "      authors.append(aut.find('Atom:name',ns).text)\n",
    "\n",
    "    papers.append({ 'id'        : paperId ,\n",
    "                    'title'     : node.find('Atom:title', ns).text , \n",
    "                    'published' : node.find('Atom:published', ns).text ,\n",
    "                    'summary'   : node.find('Atom:summary', ns).text,\n",
    "                    'categories': categories,\n",
    "                    'authors'   : authors\n",
    "                  })    \n",
    "papers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase, RoutingControl # Python database driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DB_ULR = \"neo4j://localhost:7687\"\n",
    "DB_USER = \"neo4j\"\n",
    "DB_PASS = \"test1234\"\n",
    "DB_NAME = \"demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(DB_ULR, auth=(DB_USER, DB_PASS))\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[], summary=<neo4j._work.summary.ResultSummary object at 0x15dc6abe0>, keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.execute_query(\n",
    "    'create database {dbname} if not exists'.format(dbname = DB_NAME),\n",
    "    None,\n",
    "    RoutingControl.WRITE,\n",
    "    database_= 'system'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[], summary=<neo4j._work.summary.ResultSummary object at 0x11339a790>, keys=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is not really required for this small sample\n",
    "driver.execute_query(\n",
    "    'create constraint if not exists for (p:Paper) require (p.id) is node key',\n",
    "    None,\n",
    "    routing_= RoutingControl.WRITE,\n",
    "    database_= DB_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record paper={'summary': \"  Graph machine learning has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To tackle the challenge, automated graph machine learning,\\nwhich aims at discovering the best hyper-parameter and neural architecture\\nconfiguration for different graph tasks/data without manual design, is gaining\\nan increasing number of attentions from the research community. In this paper,\\nwe extensively discuss automated graph machine approaches, covering\\nhyper-parameter optimization (HPO) and neural architecture search (NAS) for\\ngraph machine learning. We briefly overview existing libraries designed for\\neither graph machine learning or automated machine learning respectively, and\\nfurther in depth introduce AutoGL, our dedicated and the world's first\\nopen-source library for automated graph machine learning. Last but not least,\\nwe share our insights on future research directions for automated graph machine\\nlearning. This paper is the first systematic and comprehensive discussion of\\napproaches, libraries as well as directions for automated graph machine\\nlearning.\\n\", 'id': 'http://arxiv.org/abs/2201.01288v1', 'authors': ['Xin Wang', 'Ziwei Zhang', 'Wenwu Zhu'], 'title': 'Automated Graph Machine Learning: Approaches, Libraries and Directions', 'categories': ['cs.LG', 'cs.AI'], 'published': '2022-01-04T18:31:31Z'}>\n",
      "<Record paper={'summary': '  Machine learning on graphs has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To solve this critical challenge, automated machine\\nlearning (AutoML) on graphs which combines the strength of graph machine\\nlearning and AutoML together, is gaining attention from the research community.\\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\\n(NAS) for graph machine learning. We further overview libraries related to\\nautomated graph machine learning and in-depth discuss AutoGL, the first\\ndedicated open-source library for AutoML on graphs. In the end, we share our\\ninsights on future research directions for automated graph machine learning.\\nThis paper is the first systematic and comprehensive review of automated\\nmachine learning on graphs to the best of our knowledge.\\n', 'id': 'http://arxiv.org/abs/2103.00742v4', 'authors': ['Ziwei Zhang', 'Xin Wang', 'Wenwu Zhu'], 'title': 'Automated Machine Learning on Graphs: A Survey', 'categories': ['cs.LG'], 'published': '2021-03-01T04:20:33Z'}>\n",
      "<Record paper={'summary': '  Graph machine learning has been extensively studied in both academia and\\nindustry. However, in the literature, most existing graph machine learning\\nmodels are designed to conduct training with data samples in a random order,\\nwhich may suffer from suboptimal performance due to ignoring the importance of\\ndifferent graph data samples and their training orders for the model\\noptimization status. To tackle this critical problem, curriculum graph machine\\nlearning (Graph CL), which integrates the strength of graph machine learning\\nand curriculum learning, arises and attracts an increasing amount of attention\\nfrom the research community. Therefore, in this paper, we comprehensively\\noverview approaches on Graph CL and present a detailed survey of recent\\nadvances in this direction. Specifically, we first discuss the key challenges\\nof Graph CL and provide its formal problem definition. Then, we categorize and\\nsummarize existing methods into three classes based on three kinds of graph\\nmachine learning tasks, i.e., node-level, link-level, and graph-level tasks.\\nFinally, we share our thoughts on future research directions. To the best of\\nour knowledge, this paper is the first survey for curriculum graph machine\\nlearning.\\n', 'id': 'http://arxiv.org/abs/2302.02926v1', 'authors': ['Haoyang Li', 'Xin Wang', 'Wenwu Zhu'], 'title': 'Curriculum Graph Machine Learning: A Survey', 'categories': ['cs.LG'], 'published': '2023-02-06T16:59:25Z'}>\n",
      "<Record paper={'summary': '  Graph coarsening is a widely used dimensionality reduction technique for\\napproaching large-scale graph machine learning problems. Given a large graph,\\ngraph coarsening aims to learn a smaller-tractable graph while preserving the\\nproperties of the originally given graph. Graph data consist of node features\\nand graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening\\nmethods ignore the node features and rely solely on a graph matrix to simplify\\ngraphs. In this paper, we introduce a novel optimization-based framework for\\ngraph dimensionality reduction. The proposed framework lies in the unification\\nof graph learning and dimensionality reduction. It takes both the graph matrix\\nand the node features as the input and learns the coarsen graph matrix and the\\ncoarsen feature matrix jointly while ensuring desired properties. The proposed\\noptimization formulation is a multi-block non-convex optimization problem,\\nwhich is solved efficiently by leveraging block majorization-minimization,\\n$\\\\log$ determinant, Dirichlet energy, and regularization frameworks. The\\nproposed algorithms are provably convergent and practically amenable to\\nnumerous tasks. It is also established that the learned coarsened graph is\\n$\\\\epsilon\\\\in(0,1)$ similar to the original graph. Extensive experiments\\nelucidate the efficacy of the proposed framework for real-world applications.\\n', 'id': 'http://arxiv.org/abs/2210.00437v1', 'authors': ['Manoj Kumar', 'Anurag Sharma', 'Sandeep Kumar'], 'title': 'A Unified Framework for Optimization-Based Graph Coarsening', 'categories': ['stat.ML', 'cs.LG', 'math.OC'], 'published': '2022-10-02T06:31:42Z'}>\n",
      "<Record paper={'summary': '  How to utilize deep learning methods for graph classification tasks has\\nattracted considerable research attention in the past few years. Regarding\\ngraph classification tasks, the graphs to be classified may have various graph\\nsizes (i.e., different number of nodes and edges) and have various graph\\nproperties (e.g., average node degree, diameter, and clustering coefficient).\\nThe diverse property of graphs has imposed significant challenges on existing\\ngraph learning techniques since diverse graphs have different best-fit\\nhyperparameters. It is difficult to learn graph features from a set of diverse\\ngraphs by a unified graph neural network. This motivates us to use a multiplex\\nstructure in a diverse way and utilize a priori properties of graphs to guide\\nthe learning. In this paper, we propose MxPool, which concurrently uses\\nmultiple graph convolution/pooling networks to build a hierarchical learning\\nstructure for graph representation learning tasks. Our experiments on numerous\\ngraph classification benchmarks show that our MxPool has superiority over other\\nstate-of-the-art graph representation learning methods.\\n', 'id': 'http://arxiv.org/abs/2004.06846v1', 'authors': ['Yanyan Liang', 'Yanfeng Zhang', 'Dechao Gao', 'Qian Xu'], 'title': 'MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning', 'categories': ['cs.LG', 'stat.ML'], 'published': '2020-04-15T01:05:29Z'}>\n"
     ]
    }
   ],
   "source": [
    "records, summary, keys = driver.execute_query(\n",
    "    '''\n",
    "    unwind $papers as paper\n",
    "    create (p:Paper{id:paper.id})\n",
    "        set p.title=paper.title,\n",
    "            p.published=datetime(paper.published),\n",
    "            p.summary=paper.summary,\n",
    "            p.categories=paper.categories\n",
    "    foreach (author in paper.authors | \n",
    "        create (p)<-[:wrote_paper]-(:Author{name: author})\n",
    "    )\n",
    "    return paper{.*}\n",
    "    ''',\n",
    "    papers = papers,\n",
    "    routing_= RoutingControl.WRITE,\n",
    "    database_= DB_NAME\n",
    ")\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract graph from summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"sk-.....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2004.06846v1</td>\n",
       "      <td>How to utilize deep learning methods for graph classification tasks has\\nattracted considerable research attention in the past few years. Regarding\\ngraph classification tasks, the graphs to be classified may have various graph\\nsizes (i.e., different number of nodes and edges) and have various graph\\nproperties (e.g., average node degree, diameter, and clustering coefficient).\\nThe diverse property of graphs has imposed significant challenges on existing\\ngraph learning techniques since diverse graphs have different best-fit\\nhyperparameters. It is difficult to learn graph features from a set of diverse\\ngraphs by a unified graph neural network. This motivates us to use a multiplex\\nstructure in a diverse way and utilize a priori properties of graphs to guide\\nthe learning. In this paper, we propose MxPool, which concurrently uses\\nmultiple graph convolution/pooling networks to build a hierarchical learning\\nstructure for graph representation learning tasks. Our experiments on numerous\\ngraph classification benchmarks show that our MxPool has superiority over other\\nstate-of-the-art graph representation learning methods.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2103.00742v4</td>\n",
       "      <td>Machine learning on graphs has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To solve this critical challenge, automated machine\\nlearning (AutoML) on graphs which combines the strength of graph machine\\nlearning and AutoML together, is gaining attention from the research community.\\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\\n(NAS) for graph machine learning. We further overview libraries related to\\nautomated graph machine learning and in-depth discuss AutoGL, the first\\ndedicated open-source library for AutoML on graphs. In the end, we share our\\ninsights on future research directions for automated graph machine learning.\\nThis paper is the first systematic and comprehensive review of automated\\nmachine learning on graphs to the best of our knowledge.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2201.01288v1</td>\n",
       "      <td>Graph machine learning has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To tackle the challenge, automated graph machine learning,\\nwhich aims at discovering the best hyper-parameter and neural architecture\\nconfiguration for different graph tasks/data without manual design, is gaining\\nan increasing number of attentions from the research community. In this paper,\\nwe extensively discuss automated graph machine approaches, covering\\nhyper-parameter optimization (HPO) and neural architecture search (NAS) for\\ngraph machine learning. We briefly overview existing libraries designed for\\neither graph machine learning or automated machine learning respectively, and\\nfurther in depth introduce AutoGL, our dedicated and the world's first\\nopen-source library for automated graph machine learning. Last but not least,\\nwe share our insights on future research directions for automated graph machine\\nlearning. This paper is the first systematic and comprehensive discussion of\\napproaches, libraries as well as directions for automated graph machine\\nlearning.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2210.00437v1</td>\n",
       "      <td>Graph coarsening is a widely used dimensionality reduction technique for\\napproaching large-scale graph machine learning problems. Given a large graph,\\ngraph coarsening aims to learn a smaller-tractable graph while preserving the\\nproperties of the originally given graph. Graph data consist of node features\\nand graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening\\nmethods ignore the node features and rely solely on a graph matrix to simplify\\ngraphs. In this paper, we introduce a novel optimization-based framework for\\ngraph dimensionality reduction. The proposed framework lies in the unification\\nof graph learning and dimensionality reduction. It takes both the graph matrix\\nand the node features as the input and learns the coarsen graph matrix and the\\ncoarsen feature matrix jointly while ensuring desired properties. The proposed\\noptimization formulation is a multi-block non-convex optimization problem,\\nwhich is solved efficiently by leveraging block majorization-minimization,\\n$\\log$ determinant, Dirichlet energy, and regularization frameworks. The\\nproposed algorithms are provably convergent and practically amenable to\\nnumerous tasks. It is also established that the learned coarsened graph is\\n$\\epsilon\\in(0,1)$ similar to the original graph. Extensive experiments\\nelucidate the efficacy of the proposed framework for real-world applications.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2302.02926v1</td>\n",
       "      <td>Graph machine learning has been extensively studied in both academia and\\nindustry. However, in the literature, most existing graph machine learning\\nmodels are designed to conduct training with data samples in a random order,\\nwhich may suffer from suboptimal performance due to ignoring the importance of\\ndifferent graph data samples and their training orders for the model\\noptimization status. To tackle this critical problem, curriculum graph machine\\nlearning (Graph CL), which integrates the strength of graph machine learning\\nand curriculum learning, arises and attracts an increasing amount of attention\\nfrom the research community. Therefore, in this paper, we comprehensively\\noverview approaches on Graph CL and present a detailed survey of recent\\nadvances in this direction. Specifically, we first discuss the key challenges\\nof Graph CL and provide its formal problem definition. Then, we categorize and\\nsummarize existing methods into three classes based on three kinds of graph\\nmachine learning tasks, i.e., node-level, link-level, and graph-level tasks.\\nFinally, we share our thoughts on future research directions. To the best of\\nour knowledge, this paper is the first survey for curriculum graph machine\\nlearning.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2004.06846v1   \n",
       "1  http://arxiv.org/abs/2103.00742v4   \n",
       "2  http://arxiv.org/abs/2201.01288v1   \n",
       "3  http://arxiv.org/abs/2210.00437v1   \n",
       "4  http://arxiv.org/abs/2302.02926v1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   summary  \n",
       "0    How to utilize deep learning methods for graph classification tasks has\\nattracted considerable research attention in the past few years. Regarding\\ngraph classification tasks, the graphs to be classified may have various graph\\nsizes (i.e., different number of nodes and edges) and have various graph\\nproperties (e.g., average node degree, diameter, and clustering coefficient).\\nThe diverse property of graphs has imposed significant challenges on existing\\ngraph learning techniques since diverse graphs have different best-fit\\nhyperparameters. It is difficult to learn graph features from a set of diverse\\ngraphs by a unified graph neural network. This motivates us to use a multiplex\\nstructure in a diverse way and utilize a priori properties of graphs to guide\\nthe learning. In this paper, we propose MxPool, which concurrently uses\\nmultiple graph convolution/pooling networks to build a hierarchical learning\\nstructure for graph representation learning tasks. Our experiments on numerous\\ngraph classification benchmarks show that our MxPool has superiority over other\\nstate-of-the-art graph representation learning methods.\\n                                                                                                                                                                                                                                                                  \n",
       "1    Machine learning on graphs has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To solve this critical challenge, automated machine\\nlearning (AutoML) on graphs which combines the strength of graph machine\\nlearning and AutoML together, is gaining attention from the research community.\\nTherefore, we comprehensively survey AutoML on graphs in this paper, primarily\\nfocusing on hyper-parameter optimization (HPO) and neural architecture search\\n(NAS) for graph machine learning. We further overview libraries related to\\nautomated graph machine learning and in-depth discuss AutoGL, the first\\ndedicated open-source library for AutoML on graphs. In the end, we share our\\ninsights on future research directions for automated graph machine learning.\\nThis paper is the first systematic and comprehensive review of automated\\nmachine learning on graphs to the best of our knowledge.\\n                                                                                                                                                                                                                                                                               \n",
       "2    Graph machine learning has been extensively studied in both academic and\\nindustry. However, as the literature on graph learning booms with a vast number\\nof emerging methods and techniques, it becomes increasingly difficult to\\nmanually design the optimal machine learning algorithm for different\\ngraph-related tasks. To tackle the challenge, automated graph machine learning,\\nwhich aims at discovering the best hyper-parameter and neural architecture\\nconfiguration for different graph tasks/data without manual design, is gaining\\nan increasing number of attentions from the research community. In this paper,\\nwe extensively discuss automated graph machine approaches, covering\\nhyper-parameter optimization (HPO) and neural architecture search (NAS) for\\ngraph machine learning. We briefly overview existing libraries designed for\\neither graph machine learning or automated machine learning respectively, and\\nfurther in depth introduce AutoGL, our dedicated and the world's first\\nopen-source library for automated graph machine learning. Last but not least,\\nwe share our insights on future research directions for automated graph machine\\nlearning. This paper is the first systematic and comprehensive discussion of\\napproaches, libraries as well as directions for automated graph machine\\nlearning.\\n                                                                                       \n",
       "3    Graph coarsening is a widely used dimensionality reduction technique for\\napproaching large-scale graph machine learning problems. Given a large graph,\\ngraph coarsening aims to learn a smaller-tractable graph while preserving the\\nproperties of the originally given graph. Graph data consist of node features\\nand graph matrix (e.g., adjacency and Laplacian). The existing graph coarsening\\nmethods ignore the node features and rely solely on a graph matrix to simplify\\ngraphs. In this paper, we introduce a novel optimization-based framework for\\ngraph dimensionality reduction. The proposed framework lies in the unification\\nof graph learning and dimensionality reduction. It takes both the graph matrix\\nand the node features as the input and learns the coarsen graph matrix and the\\ncoarsen feature matrix jointly while ensuring desired properties. The proposed\\noptimization formulation is a multi-block non-convex optimization problem,\\nwhich is solved efficiently by leveraging block majorization-minimization,\\n$\\log$ determinant, Dirichlet energy, and regularization frameworks. The\\nproposed algorithms are provably convergent and practically amenable to\\nnumerous tasks. It is also established that the learned coarsened graph is\\n$\\epsilon\\in(0,1)$ similar to the original graph. Extensive experiments\\nelucidate the efficacy of the proposed framework for real-world applications.\\n  \n",
       "4    Graph machine learning has been extensively studied in both academia and\\nindustry. However, in the literature, most existing graph machine learning\\nmodels are designed to conduct training with data samples in a random order,\\nwhich may suffer from suboptimal performance due to ignoring the importance of\\ndifferent graph data samples and their training orders for the model\\noptimization status. To tackle this critical problem, curriculum graph machine\\nlearning (Graph CL), which integrates the strength of graph machine learning\\nand curriculum learning, arises and attracts an increasing amount of attention\\nfrom the research community. Therefore, in this paper, we comprehensively\\noverview approaches on Graph CL and present a detailed survey of recent\\nadvances in this direction. Specifically, we first discuss the key challenges\\nof Graph CL and provide its formal problem definition. Then, we categorize and\\nsummarize existing methods into three classes based on three kinds of graph\\nmachine learning tasks, i.e., node-level, link-level, and graph-level tasks.\\nFinally, we share our thoughts on future research directions. To the best of\\nour knowledge, this paper is the first survey for curriculum graph machine\\nlearning.\\n                                                                                                                                                            "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers = driver.execute_query(\n",
    "    '''\n",
    "    match (p:Paper) where p.processed is null\n",
    "    return p.id as id, p.summary as summary\n",
    "    ''',\n",
    "    None,\n",
    "    routing_= RoutingControl.READ,\n",
    "    database_= DB_NAME,\n",
    "    result_transformer_= lambda r: r.to_df()\n",
    ")\n",
    "pd.set_option('display.max_colwidth',0)\n",
    "df_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a data science expert helping us extract relevant information.\"\n",
    "\n",
    "# Set up the prompt for GPT-3 to complete\n",
    "prompt = \"\"\"#This is a research paper abstract. The task is to extract as many relevant entities to techniques, methods and applications.\n",
    "#Also, return the type of an entity using the Wikipedia class system and the sentiment of the mentioned entity,\n",
    "#where the sentiment value ranges from -1 to 1, and -1 being very negative, 1 being very positive\n",
    "#Additionally, extract all relevant relationships between identified entities.\n",
    "#The relationships should follow the Wikipedia schema type.\n",
    "#The output of a relationship should be in a form of a triple Head, Relationship, Tail, for example\n",
    "#Peter, WORKS_AT, Hospital/n\n",
    "# An example \"St. Peter is located in Paris\" should have an output with the following format\n",
    "entity\n",
    "St. Peter, person, 0.0\n",
    "Paris, location, 0.0\n",
    "\n",
    "relationships\n",
    "St.Peter, LOCATED_IN, Paris\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entities_and_relationships(input_str):\n",
    "    # Parse the input string\n",
    "    entities = []\n",
    "    relationships = []\n",
    "    entity_mode = True\n",
    "    # Skip the first line\n",
    "    for line in input_str.split(\"\\n\")[1:]:\n",
    "        if line == \"relationships\":\n",
    "            entity_mode = False\n",
    "        elif line:\n",
    "            if entity_mode:\n",
    "                # Make sure the rel is in correct format\n",
    "                # GPT-4 sometimes returns n/a when no entities are found\n",
    "                if len(line.split(\", \")) != 3:\n",
    "                    continue\n",
    "                entities.append(line.split(\", \"))\n",
    "            else:\n",
    "                # Make sure the rel is in correct format\n",
    "                # GPT-4 sometimes returns n/a when no rels are found\n",
    "                if len(line.split(\", \")) != 3:\n",
    "                    continue\n",
    "                relationships.append(line.split(\", \"))\n",
    "    return entities, relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=3, delay=5)\n",
    "def process_gpt4(text):\n",
    "    paragraph = text\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        # Try to be as deterministic as possible\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt + paragraph},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    nlp_results = completion.choices[0].message.content\n",
    "\n",
    "    print(nlp_results)\n",
    "    \n",
    "    if not \"relationships\" in nlp_results:\n",
    "        raise Exception(\n",
    "            \"GPT is not being nice and isn't returning results in correct format\"\n",
    "        )\n",
    "    \n",
    "    return parse_entities_and_relationships(nlp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entities, relationships = process_gpt4(\"\"\"How to utilize deep learning methods for graph classification tasks has\\nattracted considerable research attention in the past few years.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[], summary=<neo4j._work.summary.ResultSummary object at 0x15f81fbb0>, keys=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.execute_query(\n",
    "    'create constraint if not exists for (n:Entity) require (n.name) is node key',\n",
    "    None,\n",
    "    routing_= RoutingControl.WRITE,\n",
    "    database_= DB_NAME\n",
    ")\n",
    "driver.execute_query(\n",
    "    'create constraint if not exists for (n:Relationship) require (n.type) is node key',\n",
    "    None,\n",
    "    routing_= RoutingControl.WRITE,\n",
    "    database_= DB_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: http://arxiv.org/abs/2004.06846v1\n",
      "entities\n",
      "Deep learning methods, technique, 0.5\n",
      "Graph classification tasks, application, 0.5\n",
      "Graph sizes, property, 0.0\n",
      "Graph properties, property, 0.0\n",
      "Average node degree, property, 0.0\n",
      "Diameter, property, 0.0\n",
      "Clustering coefficient, property, 0.0\n",
      "Graph learning techniques, technique, 0.0\n",
      "Hyperparameters, method, 0.0\n",
      "Graph features, property, 0.0\n",
      "Unified graph neural network, technique, 0.0\n",
      "Multiplex structure, method, 0.0\n",
      "A priori properties, method, 0.0\n",
      "MxPool, technique, 0.5\n",
      "Multiple graph convolution/pooling networks, technique, 0.5\n",
      "Hierarchical learning structure, method, 0.5\n",
      "Graph representation learning tasks, application, 0.5\n",
      "Graph classification benchmarks, application, 0.0\n",
      "State-of-the-art graph representation learning methods, technique, 0.5\n",
      "\n",
      "relationships\n",
      "Deep learning methods, USED_FOR, Graph classification tasks\n",
      "Graph sizes, PROPERTY_OF, Graphs\n",
      "Graph properties, PROPERTY_OF, Graphs\n",
      "Average node degree, PROPERTY_OF, Graphs\n",
      "Diameter, PROPERTY_OF, Graphs\n",
      "Clustering coefficient, PROPERTY_OF, Graphs\n",
      "Graph learning techniques, CHALLENGED_BY, Diverse property of graphs\n",
      "Hyperparameters, FIT_TO, Diverse graphs\n",
      "Graph features, LEARNED_FROM, Diverse graphs\n",
      "Unified graph neural network, USED_FOR, Learning graph features\n",
      "Multiplex structure, USED_IN, Diverse way\n",
      "A priori properties, GUIDE, The learning\n",
      "MxPool, USES, Multiple graph convolution/pooling networks\n",
      "MxPool, BUILDS, Hierarchical learning structure\n",
      "Hierarchical learning structure, USED_FOR, Graph representation learning tasks\n",
      "MxPool, TESTED_ON, Graph classification benchmarks\n",
      "MxPool, COMPARED_TO, State-of-the-art graph representation learning methods\n",
      "MxPool, HAS_SUPERIORITY_OVER, State-of-the-art graph representation learning methods\n",
      "processing: http://arxiv.org/abs/2103.00742v4\n",
      "entities\n",
      "Machine learning, field of study, 0.0\n",
      "Graphs, data structure, 0.0\n",
      "Automated machine learning (AutoML), technique, 0.5\n",
      "Hyper-parameter optimization (HPO), technique, 0.5\n",
      "Neural architecture search (NAS), technique, 0.5\n",
      "AutoGL, software, 0.5\n",
      "\n",
      "relationships\n",
      "Machine learning, APPLIES_TO, Graphs\n",
      "Automated machine learning (AutoML), USED_FOR, Graphs\n",
      "Hyper-parameter optimization (HPO), PART_OF, Automated machine learning (AutoML)\n",
      "Neural architecture search (NAS), PART_OF, Automated machine learning (AutoML)\n",
      "AutoGL, IMPLEMENTS, Automated machine learning (AutoML)\n",
      "processing: http://arxiv.org/abs/2201.01288v1\n",
      "entities\n",
      "Graph machine learning, technique, 0.0\n",
      "Automated graph machine learning, technique, 0.5\n",
      "Hyper-parameter optimization (HPO), method, 0.0\n",
      "Neural architecture search (NAS), method, 0.0\n",
      "AutoGL, application, 0.5\n",
      "Open-source library, application, 0.0\n",
      "\n",
      "relationships\n",
      "Graph machine learning, HAS_METHOD, Hyper-parameter optimization (HPO)\n",
      "Graph machine learning, HAS_METHOD, Neural architecture search (NAS)\n",
      "Automated graph machine learning, USES, Hyper-parameter optimization (HPO)\n",
      "Automated graph machine learning, USES, Neural architecture search (NAS)\n",
      "AutoGL, IS_A, Open-source library\n",
      "AutoGL, USED_FOR, Automated graph machine learning\n",
      "processing: http://arxiv.org/abs/2210.00437v1\n",
      "entities\n",
      "Graph coarsening, technique, 0.0\n",
      "Dimensionality reduction, technique, 0.0\n",
      "Large-scale graph machine learning problems, application, 0.0\n",
      "Node features, method, 0.0\n",
      "Graph matrix, method, 0.0\n",
      "Optimization-based framework, technique, 0.0\n",
      "Graph learning, method, 0.0\n",
      "Block majorization-minimization, technique, 0.0\n",
      "Log determinant, method, 0.0\n",
      "Dirichlet energy, method, 0.0\n",
      "Regularization frameworks, method, 0.0\n",
      "Real-world applications, application, 0.0\n",
      "\n",
      "relationships\n",
      "Graph coarsening, USED_FOR, Large-scale graph machine learning problems\n",
      "Dimensionality reduction, USED_FOR, Large-scale graph machine learning problems\n",
      "Optimization-based framework, USES, Node features\n",
      "Optimization-based framework, USES, Graph matrix\n",
      "Optimization-based framework, USES, Block majorization-minimization\n",
      "Optimization-based framework, USES, Log determinant\n",
      "Optimization-based framework, USES, Dirichlet energy\n",
      "Optimization-based framework, USES, Regularization frameworks\n",
      "Optimization-based framework, APPLIED_TO, Real-world applications\n",
      "processing: http://arxiv.org/abs/2302.02926v1\n",
      "entities\n",
      "Graph machine learning, technique, 0.5\n",
      "Curriculum graph machine learning (Graph CL), technique, 0.5\n",
      "Node-level tasks, method, 0.0\n",
      "Link-level tasks, method, 0.0\n",
      "Graph-level tasks, method, 0.0\n",
      "Curriculum learning, technique, 0.5\n",
      "Training orders, method, -0.2\n",
      "Data samples, method, 0.0\n",
      "Model optimization status, method, 0.0\n",
      "Academia, organization, 0.0\n",
      "Industry, organization, 0.0\n",
      "Research community, organization, 0.0\n",
      "\n",
      "relationships\n",
      "Graph machine learning, USED_IN, Academia\n",
      "Graph machine learning, USED_IN, Industry\n",
      "Curriculum graph machine learning, DEVELOPED_BY, Research community\n",
      "Curriculum graph machine learning, USES, Graph machine learning\n",
      "Curriculum graph machine learning, USES, Curriculum learning\n",
      "Curriculum graph machine learning, USES, Training orders\n",
      "Curriculum graph machine learning, USES, Data samples\n",
      "Curriculum graph machine learning, USES, Model optimization status\n",
      "Curriculum graph machine learning, APPLIES_TO, Node-level tasks\n",
      "Curriculum graph machine learning, APPLIES_TO, Link-level tasks\n",
      "Curriculum graph machine learning, APPLIES_TO, Graph-level tasks\n",
      "Training orders, AFFECTS, Model optimization status\n",
      "Data samples, AFFECTS, Model optimization status\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_papers.iterrows():\n",
    "    print(\"processing: {id}\".format(id=row.id))\n",
    "    entities, relationships = process_gpt4(row.summary)\n",
    "    driver.execute_query(\n",
    "    '''\n",
    "    MATCH (p:Paper{id:$id}) set p.processed=datetime()\n",
    "    FOREACH (e in $entities |\n",
    "        MERGE (entity:Entity {name: e[0]})\n",
    "        ON CREATE SET entity.type = e[1] \n",
    "        MERGE (p)-[:MENTIONS{sentiment:toFloat(e[2])}]->(entity)\n",
    "    )\n",
    "    WITH p\n",
    "    UNWIND $relationships AS relation\n",
    "    MERGE (source:Entity {name: relation[0]})\n",
    "    MERGE (target:Entity {name: relation[2]})\n",
    "    MERGE (r:Relationship {type: relation[1]})\n",
    "    MERGE (source)-[:RELATIONSHIP]->(r)-[:RELATIONSHIP]->(target)\n",
    "    MERGE (p)-[mr:MENTIONS_RELATIONSHIP]->(r)\n",
    "    ''',\n",
    "    id = row.id,\n",
    "    relationships = relationships,\n",
    "    entities = entities,\n",
    "    routing_ = RoutingControl.WRITE,\n",
    "    database_ = DB_NAME\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
